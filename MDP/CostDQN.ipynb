{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_q_network import DQNAgent\n",
    "from base_rl_agent_torch import ReplayMemory\n",
    "from base_neural_model import EstimatorModelBase\n",
    "import utils\n",
    "import random\n",
    "from Data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seeds for reproducibility.\n",
    "utils.fix_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11797752808988764, 0.07865168539325842, 0.15730337078651685, 0.1797752808988764, 0.09550561797752809, 0.11235955056179775, 0.09550561797752809, 0.0449438202247191, 0.06179775280898876, 0.056179775280898875]\n"
     ]
    }
   ],
   "source": [
    "env = Data()\n",
    "env.loadfile(\"sample.csv\")\n",
    "env.normalize()\n",
    "env.cluster_K_means(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 100\n",
    "MAX_STEPS = 5\n",
    "BATCH_SIZE = 32\n",
    "buffer = ReplayMemory(10000)\n",
    "\n",
    "# Initiate the agent\n",
    "model = EstimatorModelBase\n",
    "agent = DQNAgent(env, model, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, [0.4868421052631582, 0.4446640316205534, 0.5561497326203207, 0.48453608247422686, 0.3695652173913043, 0.11034482758620692, None, 0.2075471698113207, None, 0.3515358361774744, None, 0.05494505494505491, None])\n",
      "6\n",
      "0.1414213562373095\n",
      "Episode 0: 0.0414213562373095 \t 1\n",
      "(44, [0.10000000000000019, 0.0, 0.6096256684491979, 0.5360824742268042, None, None, 0.35232067510548515, 0.5471698113207547, None, 0.15358361774744028, None, None, 0.11126961483594865])\n",
      "8\n",
      "0.0\n",
      "Episode 1: -0.1 \t 0.9\n",
      "(143, [None, None, 0.4759358288770053, 0.29896907216494845, None, 0.5586206896551724, 0.540084388185654, 0.15094339622641506, 0.3817034700315458, 0.3899317406143344, 0.3577235772357724, 0.706959706959707, 0.557774607703281])\n",
      "0\n",
      "0.20498880527646596\n",
      "Episode 2: 0.10498880527646595 \t 0.81\n",
      "(13, [0.4131578947368421, None, 0.28877005347593576, 0.4072164948453609, 0.1956521739130435, 0.16206896551724137, 0.2151898734177215, None, 0.2965299684542587, None, None, 0.5494505494505495, 0.20256776034236804])\n",
      "7\n",
      "0.24807406984078606\n",
      "Episode 3: 0.14807406984078605 \t 0.7290000000000001\n",
      "(65, [0.45789473684210524, None, None, 0.2783505154639176, 0.10869565217391304, None, 0.19198312236286919, None, 0.13249211356466878, None, None, None, 0.06704707560627675])\n",
      "5\n",
      "0.1414213562373095\n",
      "Episode 4: 0.0414213562373095 \t 0.6561000000000001\n",
      "(8, [None, None, 0.3422459893048128, None, 0.17391304347826086, 0.496551724137931, None, None, None, 0.10409556313993173, 0.7317073170731707, 0.6776556776556777, None])\n",
      "8\n",
      "-0.02182798048238499\n",
      "Episode 5: -0.121827980482385 \t 0.5904900000000002\n",
      "(86, [0.16315789473684233, 0.18379446640316205, 0.6737967914438503, None, 0.1956521739130435, 0.32413793103448274, None, None, 0.2933753943217666, None, 0.7154471544715448, 0.7106227106227107, None])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1d9a24d0502a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mepisode_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_EPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Github/low_rank_recon/MDP/deep_q_network.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, max_episodes, max_steps, batch_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/low_rank_recon/MDP/deep_q_network.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, env)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "episode_scores = agent.train(env, MAX_EPISODES, MAX_STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we evaluate the trained model by taking greedy actions\n",
    "observation = env.reset() # Current observed state\n",
    "total_reward = 0\n",
    "agent.epsilon = 0\n",
    "while True:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = env.step(state, action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break;\n",
    "print(\"Test Total Reward:\", total_reward)\n",
    "\n",
    "# Saving the network\n",
    "#agent.to_pickle(\"somefile.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
